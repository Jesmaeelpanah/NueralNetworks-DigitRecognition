{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a221d5ac-97c3-4eb5-beb7-29c1401f081f"
    }
   },
   "source": [
    "# Machine Learning - Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "38f45ff8-1d5b-48e5-bd22-dd902dc3a37c"
    }
   },
   "source": [
    "This section aims to implement the feedforward NN with backpropagation to a dataset of 5000 images. Both regularized and unregularized versions of the nueral network cost function and gradient computation using the backpropagation algorithm. \n",
    "\n",
    "First step to include all libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbpresent": {
     "id": "c8f8d3d1-c582-4158-8104-ada8f00819a8"
    }
   },
   "outputs": [],
   "source": [
    "#include all libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f088945a-4205-43ae-97ff-01c19ec5cb9f"
    }
   },
   "source": [
    "First step is to load and present the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbpresent": {
     "id": "844c8ea9-686b-4fe9-b6f6-c28523b1bed4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> {'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011', '__version__': '1.0', '__globals__': [], 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), 'y': array([[10],\n",
      "       [10],\n",
      "       [10],\n",
      "       ...,\n",
      "       [ 9],\n",
      "       [ 9],\n",
      "       [ 9]], dtype=uint8)}\n"
     ]
    }
   ],
   "source": [
    "#Load and show the data\n",
    "data = loadmat('../../data/ex3data1.mat')\n",
    "print (type(data), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "299f9601-0023-4ff8-b945-048cf4f4891e"
    }
   },
   "source": [
    "\"data\" is a dictionary with two labels 'X' and 'Y'. \n",
    "Next step is to assign X and Y arrays (input and output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbpresent": {
     "id": "6b5276ca-2c26-44f3-8d5b-06e433b8107c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(5000, 400) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "print (type(X), type(y))\n",
    "print (X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "95c2f326-fd5c-4754-b04f-121cdbc1442b"
    }
   },
   "source": [
    "To better perform the classification problem, y labels need to be transformed using one-hot encoding into matrix of binary values (0, 1). More explanasion on one-hot encoding can be found here:\n",
    "https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbpresent": {
     "id": "fea19630-bc6d-47e3-9db6-d9667c6d9886"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_onehot = enc.fit_transform(y)\n",
    "\n",
    "print (type(y_onehot), y_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbpresent": {
     "id": "d8fe3c51-7cf4-41a7-bc11-60fb92d8ea6b"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Test one label:\n",
    "print (y[1000], y_onehot[1000,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weight_generator(size, offset):\n",
    "    w = np.random.rand(size) * 2 * offset - offset\n",
    "    return w\n",
    "\n",
    "def act_fn(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def grad_act_fn(z):\n",
    "    return np.multiply(act_fn(z), (1 - act_fn(z)))\n",
    "\n",
    "    \n",
    "def feedforwardprop(x, w1, w2):\n",
    "    # Add bias to the first element of X and called in a1 matrix (new input matrix)\n",
    "    a1 = np.insert(x, 0, values=np.ones(1), axis=1)\n",
    "    \n",
    "    z2 = a1 * w1.T\n",
    "    a2 = np.insert(act_fn(z2), 0, values=np.ones(1), axis=1)\n",
    "    \n",
    "    z3 = a2 * w2.T\n",
    "    h = act_fn(z3)\n",
    "        \n",
    "    return a1, z2, a2, z3, h\n",
    "    \n",
    "def costfn(y, h, w1, w2, n_inputs, learning_rate, iregularization):\n",
    "    y = np.matrix(y)\n",
    "    m = n_inputs\n",
    "        \n",
    "    # calculate the cost\n",
    "    J = 0\n",
    "    for i in range(m):\n",
    "        first_log = np.multiply(-y[i,:], np.log(h[i,:]))\n",
    "        second_log = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
    "        J += np.sum(first_log - second_log)\n",
    "    \n",
    "    J = J / m\n",
    "    \n",
    "    if iregularization == 1:\n",
    "        J += ((learning_rate /2 / m) * (np.sum(w1[:,:]**2) + np.sum(w2[:,:]**2)))\n",
    "    \n",
    "    return J    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every NN, the number of layers and neourns in each layers should be fixed first. For this example, we have one input layer, one output layer and one or more hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input Parameters:\n",
    "n_samples= X.shape[0]\n",
    "n_inputs = X.shape[1]\n",
    "n_outputs= y_onehot.shape[1]\n",
    "n_hidden_layers = 1\n",
    "n_neurons= 25\n",
    "learning_rate = 1\n",
    "iregularization = 0\n",
    "\n",
    "#cost = costfn(y, h, w1, w2, n_inputs, learning_rate, iregularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n"
     ]
    }
   ],
   "source": [
    "def backpropg(X, y, n_neurons, learning_rate, iregularization):\n",
    "    m = X.shape[0]    #number of samples\n",
    "    n = X.shape[1]    #number of inputs\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    #generate a random weight matrix (-offset, +offset) and then reshape\n",
    "    offset = np.sqrt(6)/np.sqrt(n_neurons + n_inputs)\n",
    "    w1 = random_weight_generator(n_neurons * (n_inputs + 1), offset)\n",
    "    w1 = np.matrix(np.reshape(w1[:n_neurons * (n_inputs + 1)], (n_neurons, n_inputs + 1)))   #(25, 401)\n",
    "\n",
    "    offset = np.sqrt(6)/np.sqrt(n_neurons + n_outputs)\n",
    "    w2 = random_weight_generator(n_outputs * (n_neurons + 1), offset)\n",
    "    w2 = np.matrix(np.reshape(w2[:n_neurons * (n_inputs + 1)], (n_outputs, n_neurons + 1)))  #(10, 26)\n",
    "    \n",
    "    # initializations of Î”s \n",
    "    Del1 = np.zeros(w1.shape)   #(25, 401)\n",
    "    Del2 = np.zeros(w2.shape)   #(10, 26)\n",
    "\n",
    "    for i in range(2):\n",
    "        #set the input value to last sample\n",
    "        x = X[i, :]\n",
    "\n",
    "        # run the feed-forward fn\n",
    "        a1, z2, a2, z3, h = feedforwardprop(x, w1, w2)  # a1(1, 401) z2(1, 25) a2(1, 26) z3(1, 10) h(1, 10)\n",
    "        \n",
    "        del3 = h - y[i, :]   # (1, 10)\n",
    "        \n",
    "        z2p = np.insert(z2, 0, values=np.ones(1), axis=1)       # (26, 1)\n",
    "        \n",
    "        del2 = np.multiply((w2.T * del3.T).T, grad_act_fn(z2p)) # (1, 26)\n",
    "        del2 = np.delete(del2, 0, None)                         # (1, 25)\n",
    "                \n",
    "        Del1 = Del1 + np.multiply(del2.T, a1) # (25, 401)\n",
    "        Del2 = Del2 + np.multiply(del3.T, a2) # (10, 26)\n",
    "\n",
    "    dJdw1 = Del1 / m  # (25, 401)\n",
    "    dJdw2 = Del2 / m  # (10, 26)\n",
    "    \n",
    "    dJdw = np.concatenate((np.ravel(dJdw1), np.ravel(dJdw2)))\n",
    "    \n",
    "    return \n",
    "        \n",
    "        \n",
    "    \n",
    "backpropg(X, y_onehot, n_neurons, learning_rate, iregularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
